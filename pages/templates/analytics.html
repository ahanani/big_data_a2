{% extends "base.html" %} {% load static %} {% block content %}
<h4>Data Treatment</h4>
<p>
  In the process of preparing the dataset, we refrained from both imputing
  missing values and dropping any columns because the dataset had no missing
  values. The sole exception to this was with regards to the
  <b>Income</b> column, which was subjected to a specific modification - the
  reasoning for which will be clarified in subsequent analysis.
</p>
<p>
  Within the provided dataset, a diverse range of both numeric and non-numeric
  values have been identified. Specifically, the non-numeric data points are
  composed of the following:
</p>
<iframe class="df" src="{% url 'numeric_df' %}"></iframe>
<p>
  Upon analyzing the presented table, a notable observation is that the majority
  of customers are under the age of 52, with the youngest customer being a mere
  25 years old. Additionally, roughly 75% of the customer population exhibits an
  income level of $70,000 or less.
</p>
<img
  src="{% static 'resources/income_boxplot.png' %}"
  alt="Boxplot"
/><br /><br />
<p>
  It is worth noting that the <b>Income</b> column displays some outliers, as
  shown above. As such, in our production model, we leveraged percentiles as a
  means of identifying and subsequently eliminating these outliers.
</p>
<p>
  To gain further insight into the non-numeric data within our dataset, let us
  now examine a comprehensive summary of the following table:
</p>
<iframe class="df" src="{% url 'non_numeric_df' %}"></iframe>
<p>
  On close inspection, it becomes apparent that the majority of survey
  respondents hold professional positions (approximately 27%), have a commute
  distance of 0 to 1 mile (about 36%), and possess a bachelor's degree
  (approximately 30%).
</p>
<p>
  After conducting extensive analysis through the application of both reversed
  feature elimination and forward feature selection algorithms, we have
  identified a set of variables that stand out as particularly significant.
  These variables include <b>Gender</b>, <b>Income</b>, <b>Education</b>,
  <b>Occupation</b>, <b>Commute Distance</b>, and <b>Age</b>.
</p>
<h4>Target Variable</h4>
<p>
  Now, let's take a look at our target variable which is <b>Purchased Bike</b>.
  The target vriable determines if the customer baught a bike from the shop or
  not.
</p>
<img
  src="{% url 'hist_view' %}"
  alt="Histogram of target variable"
/><br /><br />
<p>
  As is evident from the figure above, an equal number of individuals can be
  observed who have purchased a bike, as those who have not.
</p>
<h4>Correlation</h4>
<p>
  The heatmap provided below represents a highly informative visualization of
  the dataset, providing a clear and concise illustration of the correlations
  that exist between different variables and the target variable.
</p>
<img src="{% url 'heatmap_view' %}" alt="Heatmap" /><br /><br />
<p>
  The aforementioned figure provides insightful correlations between the target
  variable and other pertinent factors. Notably, the target variable exhibits a
  strong negative correlation with <b>Cars</b>, <b>Commute Distance</b>, and
  <b>Children</b>, while a positive correlation can be observed between
  <b>Cars</b> and <b>Income</b>, and between <b>Children</b> and <b>Age</b>.
</p>
<p>
  Moreover, the visualization also highlights an intriguing negative correlation
  between <b>Home Owner</b> and <b>Marital Status</b>, which can have
  implications on the interpretation and application of the model's results.
  Altogether, the heatmap presented provides valuable insights into the dataset
  and can assist in making informed decisions during the model-building process.
</p>
<h4>Model Evaluation</h4>
<p>
  After careful evaluation of the predictions generated by the various models
  utilized, it is noteworthy that the <b>RandomForestClassifier</b> model
  emerged as the most effective in terms of RMSE.
</p>
<table>
  <thead>
    <tr>
      <th></th>
      <th>RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LogisticRegression</th>
      <td>0.5586</td>
    </tr>
    <tr>
      <th>DecisionTreeClassifier</th>
      <td>0.5859</td>
    </tr>
    <tr>
      <th>SVC</th>
      <td>0.4727</td>
    </tr>
    <tr>
      <th>RandomForestClassifier</th>
      <td>0.6367</td>
    </tr>
  </tbody>
</table>
<br /><br />
<p>
  This is particularly significant given that RMSE is a robust and widely
  accepted metric that provides a clear indication of a model's accuracy and
  performance. By measuring the difference between the predicted and actual
  values in a dataset, RMSE offers a reliable gauge of the quality of a model's
  fit, with lower scores indicating a better fit.
</p>
{% endblock content %}
